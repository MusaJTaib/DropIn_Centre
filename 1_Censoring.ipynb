{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook 1 for clients data\n",
    "<br>\n",
    "<em> Modified GettingStarted notebook from the master branch to censor/redact clients on the basis of their first sleep date and subsequently filtering the clients first n days of records(n = 30 , 60, 90, 120, 150, 90 days) . The first sleep date is the first Stay of a client in the database. </em> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Data Analytics Utility Functions of GettingStarted\n",
    "- utility code found in `util/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Standard routines for pre-processing and analyzing client data from the Calgary\n",
    "Drop-In Centre.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def RemoveByStartDate(tbl,winStartDate,winEndDate,dateSelect = pd.Series(dtype='object')):\n",
    "    \"\"\"Remove all records for subjects in tbl first appearing in the data between\n",
    "    winStartDate and winEndDate (determined using tbl.Date values).\"\"\"\n",
    "\n",
    "    if dateSelect.empty:\n",
    "        tblFlt = tbl\n",
    "    else:\n",
    "        tblFlt = tbl.loc[dateSelect]\n",
    "        \n",
    "    startDates = tblFlt.groupby('ClientId').apply(lambda x: min(x.Date))\n",
    "    notCensored = ~ ((startDates >= winStartDate) & (startDates <= winEndDate ))\n",
    "    \n",
    "    return tbl.loc[tbl.ClientId.isin(startDates[notCensored].index)]    \n",
    "\n",
    "def ShelterGroupDemographics(tbl):\n",
    "    \"\"\"Summarizes the demographics of a group of shelter clients.\n",
    "    - Fields:\n",
    "     > TotalStays: Total number of shelter stays.\n",
    "     > Tenure: Number of days between first and last appearance in dataset.\n",
    "     > UsagePct: Percentage of days during tenure spent in shelter.\n",
    "     > AvgGapLen: Average length of gaps between shelter stays (days).\n",
    "                  NaN for clients with a single stay.\n",
    "     > TotalEpisodes: Total number of episodes of shelter access.\n",
    "     \"\"\"\n",
    "    \n",
    "    dates = tbl.Date.drop_duplicates().sort_values() \n",
    "    tl = pd.DataFrame({\n",
    "        'Date': dates,                \n",
    "        'Ind': range(1,len(dates)+1)  \n",
    "        })\n",
    "    \n",
    "    tenure = (tl.Date.max() - tl.Date.min()).days + 1\n",
    "    gapVals = tl.Date.diff().astype('timedelta64[D]')\n",
    "    nStays = tl.Ind.max()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Tenure': tenure,  # Total span of days a client interacts with shelter.\n",
    "        'UsagePct': 100.0*nStays/tenure,  # Percentage of days during tenure client stayed in shelter.\n",
    "        'AvgGapLen': gapVals.mean(),  # Average length of gaps in shelter stays.\n",
    "        'TotalStays': nStays,  # Total number of shelter stays.\n",
    "        'TotalEpisodes': sum(gapVals >= episodeGap)+1  # Total number of episodes.\n",
    "    })\n",
    "\n",
    "def CalculateStaySequence(tbl):\n",
    "    \"\"\"Determines a stay timeline for a subject.\n",
    "    - Each event in the timeline is represented by an index and a timestamp.\n",
    "    - A stay is defined as accessing one or more services (typically sleep services) \n",
    "      in a 24 hour period.\n",
    "    - Timestamps generated using tbl.Date values.\"\"\"\n",
    "    \n",
    "    dates = tbl.Date.drop_duplicates().sort_values() # Drop duplicates since stay is one or more sleep.\n",
    "    return pd.DataFrame({\n",
    "        'Date': dates,                 # Date of each stay.\n",
    "        'Ind': range(1,len(dates)+1)   # Index of each stay.\n",
    "    })\n",
    "\n",
    "\n",
    "episodeGap = 30  # The max gap in stays before a new episode is created.\n",
    "\n",
    "def CalculateEpisodeSequence(tbl):    \n",
    "    \"\"\"Determines an episode timeline for a subject.  \n",
    "    - Each event in the timeline is represented by an index and a timestamp.\n",
    "    - An episode is a series of shelter stays separated by gaps of less than \n",
    "      di_data.episodeGap days.\n",
    "    - A stay is defined as accessing one or more services (typically sleep services) \n",
    "      in a 24 hour period.\n",
    "    - Timestamps generated using tbl.Date values.\"\"\"\n",
    "    \n",
    "    stayDates = tbl.Date.drop_duplicates().sort_values() # Drop duplicates since stay is one or more sleep.\n",
    "    gapVals = stayDates.diff().astype('timedelta64[D]')\n",
    "    gapInd = (gapVals >= episodeGap).astype('int').cumsum().drop_duplicates(keep='first')\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Date': tbl.loc[gapInd.index].Date, # Date of first day of each episode.\n",
    "        'Ind': range(1,len(gapInd)+1)       # Episode index.\n",
    "    })\n",
    "\n",
    "\n",
    "def TimeWinThresholdTest(tbl,posFlag,negFlag,thresh,winSzDays):\n",
    "    \"\"\"Analyze a subject timeline and determine if the number of events\n",
    "    exceed thresh in a time window of winSzDays.\n",
    "    - idDate is the date the threshold test is satisfied.\n",
    "    - reqTime is the number of days it took to satisfy the threshold test.\n",
    "    - If the test is satisfied, return a series with posFlag, idDate and reqTime.\n",
    "    - If the test is not satisfied, return a series with negFlag and nan values\n",
    "      for idDate and reqTime.\"\"\"\n",
    "    \n",
    "    win = tbl.rolling('{:d}d'.format(winSzDays),on='Date').count().Ind\n",
    "    \n",
    "    registrationDate = tbl.Date.min()\n",
    "    idDate = tbl[win >= thresh].Date.min()  # Will be equal to NaN if the threshold isn't met.\n",
    "    reqTime = (idDate - registrationDate).days\n",
    "    \n",
    "    if idDate == idDate:   # Satisfied if idDate is not NaN.\n",
    "        return pd.Series({\n",
    "            'Flag': posFlag,  \n",
    "            'Date': idDate,  # Date subject was identified.\n",
    "            'Time': reqTime  # Number of days it took to identify subject.\n",
    "        })\n",
    "    else:\n",
    "        return pd.Series({   # Returned if the test is not satisfied.\n",
    "            'Flag': negFlag,\n",
    "            'Date': pd.NaT,\n",
    "            'Time': np.nan\n",
    "        })\n",
    "\n",
    "    \n",
    "def ChooseEarliestTest(test1,test2):\n",
    "    \"\"\"Merges two test tables.  If each test is positive for a subject, the test that\n",
    "    occurs earliest in a subject's timeline is chosen.  \n",
    "    \n",
    "    If you have more than two test tables, you can call this routine several times.  For \n",
    "    example, tables A, B and C can be merged by:\n",
    "      mrg = ChooseEarliestTest(A,B)\n",
    "      mrg = ChooseEarliestTest(mrg,C)\n",
    "      \n",
    "    Assumptions:\n",
    "    - Both test tables contain the identical list of subjects.\n",
    "    - Both tests use the same flag for a negative result.\n",
    "    \"\"\"\n",
    "    nRec = len(test1.index)\n",
    "    \n",
    "    tbl = pd.DataFrame({ 'Flag': ['']*nRec, 'Date': [pd.NaT]*nRec, 'Time': [np.nan]*nRec },index=test1.index)\n",
    "\n",
    "    bothNeg = (test1.Time != test1.Time) & (test2.Time != test2.Time)\n",
    "    tbl[bothNeg] = test1[bothNeg]\n",
    "    \n",
    "    isOne = (test1.Time == test1.Time) & (test2.Time == test2.Time) & (test1.Time < test2.Time)\n",
    "    isOne = isOne | ( (test1.Time == test1.Time) & (test2.Time != test2.Time) )\n",
    "    tbl[isOne] = test1[isOne]\n",
    "    \n",
    "    isTwo = (test1.Time == test1.Time) & (test2.Time == test2.Time) & (test1.Time >= test2.Time)\n",
    "    isTwo = isTwo | ( (test2.Time == test2.Time) & (test1.Time != test1.Time) )\n",
    "    tbl[isTwo] = test2[isTwo]\n",
    "    \n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime, copy, imp\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Style\n",
    "- In general, we will follow the [CamelCase](https://en.wikipedia.org/wiki/Camel_case) naming convention.\n",
    "- Functions, classes, data types and pandas column names will follow upper camel case or Pascal case where the first letter is capitalized (ie. PrintOutput(), ReturnResults()).\n",
    "- Variable names will follow lower camel case or Dromedary case where the first letter is not capitalized (ie. myVariable, functionReturnValue).\n",
    "- Notebook names should be descriptive and can use typical sentence capitalization and spacing.\n",
    "- Module names should be all lower case with underscores optionally used for word separation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Censoring\n",
    "When analyzing event timelines, our data will always have a finite window with a start and stop time. *Censoring* refers to when a value occurs that is outside the range of a measuring instrument.  For a study with a finite time window:\n",
    "- *Left Censoring*: Occurs when a subject's activity starts before the study window.  \n",
    "- *Right Censoring*: Occurs when a subject's activity continues after the end of the study window.\n",
    "\n",
    "If mitigating censoring is important, a buffer period can be defined at the start and end of the study window.  Any subjects with time records in the start buffer are removed (addresses left censoring) and any subjects with time records in the end buffer are removed.\n",
    "\n",
    "**IMPORTANT:** All data records for the subjects that appear in the start/end buffers are removed, not just the records that occur in the buffer.  Censored subjects must be removed entirely from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(RemoveByStartDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "- In general, you will likely need to develop a pre-processing routine that is specific to your research.  It will take in the master dataset and perform a series of modifications to the data that all of your routines will require.  In this case, decorating your pre-processing function with the local caching routine CacheResult will prevent the pre-processing routine from having to run each time you execute your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pre-processing example is for a study that utilizes machine learning to predict chronic homelessness based on client age and number of sleep stays in shelter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessSleepEntries():\n",
    "    \n",
    "    tbl = pd.read_hdf('UniversityExportAnonymized.hd5')\n",
    "    tbl.drop_duplicates(subset=['ClientId', 'EntryType', 'Date'], keep='first', inplace=True)\n",
    "    tbl = tbl[tbl.EntryType == 'Sleep'][['ClientId','Date','EntryType']]\n",
    "    \n",
    "    leftStart = tbl.Date.min()\n",
    "    leftEnd = pd.to_datetime('2009-07-01')\n",
    "    rightStart = pd.to_datetime('2017-07-31')\n",
    "    rightEnd = tbl.Date.max()\n",
    "    \n",
    "    nClientsAll = len(tbl.ClientId.unique())\n",
    "    \n",
    "    tbl = RemoveByStartDate(tbl,leftStart,leftEnd)\n",
    "    nLeftRemoved = nClientsAll - len(tbl.ClientId.unique())\n",
    "\n",
    "    tbl = RemoveByStartDate(tbl,rightStart,rightEnd)\n",
    "    nRightRemoved = nClientsAll - nLeftRemoved - len(tbl.ClientId.unique())\n",
    "\n",
    "    nClients = len(tbl.ClientId.unique())\n",
    " \n",
    "    print('Total Valid Clients: {:d}/{:d} ({:d} removed left, {:d} removed right)'\n",
    "          .format(nClients,nClientsAll,nLeftRemoved,nRightRemoved))\n",
    "\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = PreProcessSleepEntries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe housing only contains the dates of sleep entries of the valid clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShelter = pd.read_hdf('UniversityExportAnonymized.hd5')    \n",
    "totalClients = len(dfShelter.ClientId.unique())\n",
    "print('Total Clients:',totalClients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Sleep Date\n",
    " \n",
    "- It is seprate from Registration Date. It is the first time a client stays at the DI-shelter and accesses Day/Night sleep facilities (or both)\n",
    "- Registration date is defined seprately in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindFirstSleepDate(tbl):\n",
    "    return min(tbl.Date)\n",
    "sleepDates = housing.groupby('ClientId').progress_apply(FindFirstSleepDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepDates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registration Date \n",
    "- Registration date is the First time a particular client accesses the DI-shelter for any service (sleep,counselor,laundry etc)\n",
    "- Registration date is the first date a client shows up in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRegistrationDate(tbl):\n",
    "    return min(tbl.Date)\n",
    "registrationDate = dfShelter.groupby('ClientId').progress_apply(FindRegistrationDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleepDates = pd.DataFrame(sleepDates)\n",
    "registrationDate = pd.DataFrame(registrationDate)\n",
    "sleepDates = sleepDates.rename(columns={0:'FirstSleepDate'})\n",
    "registrationDate = registrationDate.rename(columns={0:'RegistrationDate'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeDates = registrationDate.join(sleepDates, how ='left',on='ClientId')\n",
    "mergeDates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Clients\n",
    "- These are the valid client ID's Registration Date and First Sleep Date\n",
    "- Redaction done on the basis of first Sleep date in PreProcessSleepEntries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeDates[mergeDates.FirstSleepDate.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out Clients who never Stay at DI shelter\n",
    "- The following code filters out invalid client's shelter data who dont have even a single sleep entry on the basis of first Sleep date\n",
    "- By Replacing the NaN values of FirstSleepDate with an arbitrary date before the left-cutoff so that the clients who never access the sleep facilities of the DI Shelter are also redacted using the left-cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitaryDate = pd.to_datetime('2001-01-01')\n",
    "mergeDates = mergeDates.fillna(arbitaryDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leftCutoff = pd.to_datetime('2009-07-01')\n",
    "rightCutoff = pd.to_datetime('2017-07-31')\n",
    "validClientInd = (mergeDates.FirstSleepDate >= leftCutoff) & (mergeDates.FirstSleepDate <= rightCutoff)\n",
    "nClientsAll = 34577\n",
    "nClients = len(sleepDates)\n",
    "print('Original Clients: %d'% (totalClients))\n",
    "print('Total clients who access Sleep facilities: %d \\nValid Clients: %d \\nValid Clients per original clients: (%.1f%%) \\nValid Clients per total clients acessing sleep facilities: (%.1f%%)' % (nClientsAll,nClients,100*nClients/totalClients,100*nClients/nClientsAll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redaction (Censoring) of the clients records on the basis of their first sleep date\n",
    "- Include only valid clients that meet the left/right censoring cutoff criteria and access DI: all entry types including sleep, counsellor, logs etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf = dfShelter.loc[list(validClientInd[dfShelter.ClientId])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validClientsDf.ClientId.unique()) # To verify check if this number matches the number of Valid Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf.drop_duplicates(subset=['ClientId', 'EntryType', 'Date'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = validClientsDf.sort_values(['ClientId', 'Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Saving the records of the censored clients to the disk</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf.loc[:,'Location'] = validClientsDf['Location'].astype(str)\n",
    "validClientsDf.loc[:,'EntryType'] = validClientsDf['EntryType'].astype(str)\n",
    "validClientsDf.loc[:,'ClientState'] = validClientsDf['ClientState'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = validClientsDf.sort_values(['ClientId', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "start_date = pd.to_datetime('2009-07-01')\n",
    "end_date = pd.to_datetime('2017-07-31')\n",
    "mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
    "filtered_df = df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_hdf('validClientsDf.h5', key='df', mode='w')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f3ba9e726005c067fbc0eaab20e31f21cd7f7e132fec231b7dd4c5a2981fed5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
