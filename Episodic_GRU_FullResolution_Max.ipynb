{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Windows: 1,2,3,6,12,22,45,90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfWindows = 90\n",
    "Restriction = '90'\n",
    "ep = 125\n",
    "numberOfFeatures = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "los_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB2 = pd.DataFrame()\n",
    "FeatureSetB3 = pd.DataFrame()\n",
    "FeatureSetB4 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB1 = pd.read_csv('DI/DATA_Base.csv') #Split\n",
    "FeatureSetB2 = pd.read_csv(f'DI/DATA_{numberOfWindows}.csv')#Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB1.drop(\"Unnamed: 0\",axis=1,inplace = True)\n",
    "FeatureSetB2.drop(\"Unnamed: 0\",axis=1,inplace = True)\n",
    "\n",
    "FeatureSetB1 = FeatureSetB1.fillna(0)\n",
    "FeatureSetB2 = FeatureSetB2.fillna(0)\n",
    "\n",
    "FeatureMortality2 = pd.read_csv('DI/Sleep_Tbl.csv') #Labels\n",
    "FeatureMortality2.drop(\"Unnamed: 0\",axis=1,inplace = True)\n",
    "FeatureMortality = pd.DataFrame()\n",
    "FeatureMortality['subject_id'] = FeatureMortality2['ClientId']\n",
    "FeatureMortality['Flags'] = FeatureMortality2['Flags']\n",
    "\n",
    "FeatureSetB1.sort_values(by=['subject_id'], ascending=True,inplace=True)\n",
    "\n",
    "FeatureMortality.sort_values(by=['subject_id'], ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = pd.DataFrame()\n",
    "subject_id = FeatureSetB1.subject_id\n",
    "subject_id.drop_duplicates(keep = 'first', inplace = True)\n",
    "subject_id.reset_index(drop=True,inplace=True)\n",
    "NumSubjects = len(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureMortality['subject_id'].isin(subject_id)\n",
    "FeatureMortality = FeatureMortality[X]\n",
    "FeatureMortality.sort_values(by=['subject_id'], ascending=True,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = pd.DataFrame()\n",
    "subject_id = FeatureMortality.subject_id\n",
    "subject_id.drop_duplicates(keep = 'first', inplace = True)\n",
    "subject_id.reset_index(drop=True,inplace=True)\n",
    "NumSubjects = len(subject_id)\n",
    "X = FeatureSetB1['subject_id'].isin(subject_id)\n",
    "FeatureSetB1 = FeatureSetB1[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMortality.Flags[FeatureMortality.Flags == 1] = 0\n",
    "FeatureMortality.Flags[FeatureMortality.Flags == 2] = 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureMortality['Flags'].value_counts()         #Chronic is 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = pd.DataFrame()\n",
    "Labels['subject_id'] = FeatureMortality['subject_id']\n",
    "Labels['Flags'] = FeatureMortality['Flags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureSetB1\n",
    "X_1 = [X,y]\n",
    "\n",
    "X_2 = reduce(lambda  left,right: pd.merge(left,right,on=['subject_id'],how='outer'), X_1)\n",
    "X_2.sort_values(by=['subject_id'], ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = X_2.dropna(subset=['Flags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = pd.DataFrame()\n",
    "subject_id = X_2.subject_id\n",
    "subject_id.drop_duplicates(keep = 'first', inplace = True)\n",
    "subject_id.reset_index(drop=True,inplace=True)\n",
    "NumSubjects = len(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureMortality['subject_id'].isin(subject_id)\n",
    "FeatureMortality = FeatureMortality.reset_index(drop=True)\n",
    "FeatureMortality = FeatureMortality[X]\n",
    "FeatureMortality.sort_values(by=['subject_id'], ascending=True,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_age = X_2['age'].mean()\n",
    "X_emp = X_2['emp'].mean()\n",
    "X_emc = X_2['emc'].mean()\n",
    "X_sleep = X_2['sleep'].mean()\n",
    "X_ssAgg = X_2['ssAgg'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2.loc[(X_2.age < X_age),  'age_Group'] = 'Young'\n",
    "X_2.loc[(X_2.age >= X_age),  'age_Group'] = 'Old'\n",
    "\n",
    "X_2.loc[(X_2.emp < X_emp),  'emp_Group'] = 'Low'\n",
    "X_2.loc[(X_2.emp >= X_emp),  'emp_Group'] = 'High'\n",
    "\n",
    "X_2.loc[(X_2.emc < X_emc),  'emc_Group'] = 'Low'\n",
    "X_2.loc[(X_2.emc >= X_emc),  'emc_Group'] = 'High'\n",
    "\n",
    "X_2.loc[(X_2.sleep < X_sleep),  'sleep_Group'] = 'Low'\n",
    "X_2.loc[(X_2.sleep >= X_sleep),  'sleep_Group'] = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2['Flags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2['age_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2['emp_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2['emc_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2['sleep_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureSetB1\n",
    "\n",
    "X_train1, X_test = train_test_split(X_2, test_size=0.25, stratify=X_2[['Flags']], shuffle=True,random_state=42)#,'age_Group','emp_Group','emc_Group','sleep_Group']]#,'AIDS','MetaStatic','Hematologic'#,'GCSEyes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['Flags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['sleep_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['emc_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['emp_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1['age_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X_train1, test_size=0.2, random_state=42, stratify=X_train1[['Flags']], shuffle=True)#,'age_Group','emp_Group','emc_Group','sleep_Group']], shuffle=True)#, shuffle=True)#,'GCSEyes']],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test['Flags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val['Flags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['Flags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Subject_Test = pd.DataFrame()\n",
    "Subject_Test = X_test.subject_id\n",
    "\n",
    "Subject_Train = pd.DataFrame()\n",
    "Subject_Train = X_train.subject_id\n",
    "\n",
    "Subject_Val = pd.DataFrame()\n",
    "Subject_Val = X_val.subject_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureMortality['subject_id'].isin(Subject_Train)\n",
    "y_train = FeatureMortality[X]\n",
    "\n",
    "X = FeatureMortality['subject_id'].isin(Subject_Test)\n",
    "y_test = FeatureMortality[X]\n",
    "\n",
    "X = FeatureMortality['subject_id'].isin(Subject_Val)\n",
    "y_val = FeatureMortality[X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sort_values(by=['subject_id'], ascending=True,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = FeatureSetB2['subject_id'].isin(Subject_Train)\n",
    "X_train = FeatureSetB2[X]\n",
    "\n",
    "\n",
    "X = FeatureSetB2['subject_id'].isin(Subject_Test)\n",
    "X_test = FeatureSetB2[X]\n",
    "\n",
    "\n",
    "X = FeatureSetB2['subject_id'].isin(Subject_Val)\n",
    "X_val = FeatureSetB2[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_shape = X_train.shape\n",
    "Test_shape = X_test.shape\n",
    "Val_shape = X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train.drop(\"subject_id\",axis=1,inplace = True)\n",
    "\n",
    "\n",
    "X_test.drop(\"subject_id\",axis=1,inplace = True)\n",
    "\n",
    "\n",
    "X_val.drop(\"subject_id\",axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "\n",
    "X_test = X_test.to_numpy()\n",
    "\n",
    "X_val = X_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = StandardScaler().fit(X_train)\n",
    "scaler2 = StandardScaler().fit(X_test)\n",
    "scaler3 = StandardScaler().fit(X_val)\n",
    "\n",
    "X_train = scaler1.transform(X_train)\n",
    "X_test = scaler2.transform(X_test)\n",
    "X_val = scaler3.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(int(Train_shape[0]/numberOfWindows),numberOfWindows,numberOfFeatures)\n",
    "X_test = X_test.reshape(int(Test_shape[0]/numberOfWindows),numberOfWindows,numberOfFeatures)\n",
    "X_val = X_val.reshape(int(Val_shape[0]/numberOfWindows),numberOfWindows,numberOfFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train4 = X_train\n",
    "\n",
    "X_test4 = X_test\n",
    "\n",
    "X_val4 = X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['Flags']\n",
    "y_train = y_train.to_numpy()\n",
    "y_train = y_train.flatten()\n",
    "\n",
    "y_test = y_test['Flags']\n",
    "y_test = y_test.to_numpy()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "y_val = y_val['Flags']\n",
    "y_val = y_val.to_numpy()\n",
    "y_val = y_val.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Flatten, Activation\n",
    "from sklearn import metrics\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "mod=sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 45, mode='min',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "acc_score = []\n",
    "re_score = []\n",
    "pre_score = []\n",
    "history_1 = []\n",
    "f_score = []\n",
    "acc_macro_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfFeatures2 = X_test4.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_size = sum(y_train.astype(int) == 1)\n",
    "majority_size = int(minority_size * 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for x in range(40):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(GRU(256, return_sequences=True, input_shape=(numberOfWindows ,numberOfFeatures)))  # Adjust the input shape depending on your data's time steps and features\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    opt = tf.keras.optimizers.legacy.Adam()  # If you need specific learning rate or other parameters, define them here.\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    history = model.fit(X_train4, y_train, epochs=ep, batch_size=1024, validation_data=(X_val4, y_val), \n",
    "                        callbacks=[stop_early], shuffle=True, class_weight=class_weights)\n",
    "    \n",
    "    y_preds = model.predict(X_test4)\n",
    "    y_preds = y_preds.flatten()\n",
    "    y_pred = [1 if pred >= 0.5 else 0 for pred in y_preds]  # a simpler way to apply the threshold\n",
    "\n",
    "    y_actu = pd.Series(y_test)\n",
    "    y_pred = pd.Series(y_pred)\n",
    "\n",
    "    sensitivity = recall_score(y_actu, y_pred, average='macro')\n",
    "    precision = precision_score(y_actu, y_pred, average='macro')\n",
    "    f1_value = f1_score(y_actu, y_pred, average='macro')\n",
    "    accuracy = accuracy_score(y_actu, y_pred)\n",
    "\n",
    "    acc_score.append(accuracy)\n",
    "    re_score.append(sensitivity)\n",
    "    pre_score.append(precision)\n",
    "    f_score.append(f1_value)\n",
    "\n",
    "    matrix = confusion_matrix(y_actu, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix).plot()  # This will plot the confusion matrix. Use plt.show() if you're running this in a script.\n",
    "    Accuracies = matrix.diagonal() / matrix.sum(axis=1)\n",
    "    accuracy_macro = np.mean(Accuracies)  # assuming you have 2 classes, this calculates the mean\n",
    "    acc_macro_score.append(accuracy_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 40\n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_recall_score = sum(re_score)/k\n",
    "avg_precision_score = sum(pre_score)/k\n",
    "avg_f1_score = sum(f_score)/k\n",
    "avg_acc_macro_score = sum(acc_macro_score)/k\n",
    "\n",
    "accuracy_macro = avg_acc_macro_score\n",
    "sensitivity = avg_recall_score\n",
    "precision = avg_precision_score\n",
    "accuracy = avg_acc_score\n",
    "f1_score = avg_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_standard_deviation(numbers):\n",
    "    n = len(numbers)\n",
    "    mean = sum(numbers) / n\n",
    "    variance = sum((x - mean) ** 2 for x in numbers) / n\n",
    "    std_dev = math.sqrt(variance)\n",
    "    return std_dev\n",
    "\n",
    "std_acc = calculate_standard_deviation(acc_score)\n",
    "std_acc_macro = calculate_standard_deviation(acc_macro_score)\n",
    "std_recall = calculate_standard_deviation(re_score)\n",
    "std_precision = calculate_standard_deviation(pre_score)\n",
    "std_f1_score = calculate_standard_deviation(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_min = min(re_score)\n",
    "sensitivity_max = max(re_score)\n",
    "\n",
    "precision_min = min(pre_score)\n",
    "precision_max = max(pre_score)\n",
    "\n",
    "accuracy_min = min(acc_score)\n",
    "accuracy_max = max(acc_score)\n",
    "\n",
    "accuracy_macro_min = min(acc_macro_score)\n",
    "accuracy_macro_max = max(acc_macro_score)\n",
    "\n",
    "f1_score_min = min(f_score)\n",
    "f1_score_max = max(f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "print('Avg Reccall : {}'.format(avg_recall_score))\n",
    "print('Avg Precision : {}'.format(avg_precision_score))\n",
    "print('Avg F1_score : {}'.format(avg_f1_score))\n",
    "print('Avg accuracy_macro : {}'.format(avg_acc_macro_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in history_1:\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for history in history_1:\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Validation F1')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "\n",
    "csv_columns = ['model-type','precision','sensitivity','f1-score','accuracy','accuracy_macro','NumberOfWindows','Epochs','Run_Time','Restriction','Acc_Lesser','Acc_Greater']\n",
    "dict_data = [{'model-type':'ANN', 'precision': precision,'sensitivity': sensitivity,'f1-score': f1_score,'accuracy': accuracy,'accuracy_macro': accuracy_macro,'NumberOfWindows':numberOfWindows,\"Epochs\":ep,'Run_Time':runtime,'Restriction' : Restriction,'Acc_Lesser':Less10D,'Acc_Greater':Greater10D}]\n",
    "metric_file = \"Results/Episodic_ANN_Max_FR.csv\"\n",
    "\n",
    "file_exists = os.path.isfile(metric_file)\n",
    "try:\n",
    "    with open(metric_file, 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
