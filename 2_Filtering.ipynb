{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Notebook 2 for filtering first 90days of clients data\n",
    "<p> <em>The first 90 days of shelter access including sleeps,logs,counsellor notes etc are filtered for each client.<br>Filtering is performed from the Registration date or the first DI shelter access date on the already censored raw data obtained in Notebook 1</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import scipy as sci\n",
    "import scipy.special as scisp\n",
    "import scipy.stats as scist\n",
    "import datetime, copy, imp, sys\n",
    "sys.path.append('../../lib')\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Standard routines for pre-processing and analyzing client data from the Calgary\n",
    "Drop-In Centre.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def RemoveByStartDate(tbl,winStartDate,winEndDate,dateSelect = pd.Series(dtype='object')):\n",
    "    \"\"\"Remove all records for subjects in tbl first appearing in the data between\n",
    "    winStartDate and winEndDate (determined using tbl.Date values).\"\"\"\n",
    "\n",
    "    if dateSelect.empty:\n",
    "        tblFlt = tbl\n",
    "    else:\n",
    "        tblFlt = tbl.loc[dateSelect]\n",
    "        \n",
    "    startDates = tblFlt.groupby('ClientId').apply(lambda x: min(x.Date))\n",
    "    notCensored = ~ ((startDates >= winStartDate) & (startDates <= winEndDate ))\n",
    "    \n",
    "    return tbl.loc[tbl.ClientId.isin(startDates[notCensored].index)]    \n",
    "\n",
    "def ShelterGroupDemographics(tbl):\n",
    "    \"\"\"Summarizes the demographics of a group of shelter clients.\n",
    "    - Fields:\n",
    "     > TotalStays: Total number of shelter stays.\n",
    "     > Tenure: Number of days between first and last appearance in dataset.\n",
    "     > UsagePct: Percentage of days during tenure spent in shelter.\n",
    "     > AvgGapLen: Average length of gaps between shelter stays (days).\n",
    "                  NaN for clients with a single stay.\n",
    "     > TotalEpisodes: Total number of episodes of shelter access.\n",
    "     \"\"\"\n",
    "    \n",
    "    dates = tbl.Date.drop_duplicates().sort_values() \n",
    "    tl = pd.DataFrame({\n",
    "        'Date': dates,                \n",
    "        'Ind': range(1,len(dates)+1)  \n",
    "        })\n",
    "    \n",
    "    tenure = (tl.Date.max() - tl.Date.min()).days + 1\n",
    "    gapVals = tl.Date.diff().astype('timedelta64[D]')\n",
    "    nStays = tl.Ind.max()\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Tenure': tenure,  # Total span of days a client interacts with shelter.\n",
    "        'UsagePct': 100.0*nStays/tenure,  # Percentage of days during tenure client stayed in shelter.\n",
    "        'AvgGapLen': gapVals.mean(),  # Average length of gaps in shelter stays.\n",
    "        'TotalStays': nStays,  # Total number of shelter stays.\n",
    "        'TotalEpisodes': sum(gapVals >= episodeGap)+1  # Total number of episodes.\n",
    "    })\n",
    "\n",
    "def CalculateStaySequence(tbl):\n",
    "    \"\"\"Determines a stay timeline for a subject.\n",
    "    - Each event in the timeline is represented by an index and a timestamp.\n",
    "    - A stay is defined as accessing one or more services (typically sleep services) \n",
    "      in a 24 hour period.\n",
    "    - Timestamps generated using tbl.Date values.\"\"\"\n",
    "    \n",
    "    dates = tbl.Date.drop_duplicates().sort_values() # Drop duplicates since stay is one or more sleep.\n",
    "    return pd.DataFrame({\n",
    "        'Date': dates,                 # Date of each stay.\n",
    "        'Ind': range(1,len(dates)+1)   # Index of each stay.\n",
    "    })\n",
    "\n",
    "\n",
    "episodeGap = 30  # The max gap in stays before a new episode is created.\n",
    "\n",
    "def CalculateEpisodeSequence(tbl):    \n",
    "    \"\"\"Determines an episode timeline for a subject.  \n",
    "    - Each event in the timeline is represented by an index and a timestamp.\n",
    "    - An episode is a series of shelter stays separated by gaps of less than \n",
    "      di_data.episodeGap days.\n",
    "    - A stay is defined as accessing one or more services (typically sleep services) \n",
    "      in a 24 hour period.\n",
    "    - Timestamps generated using tbl.Date values.\"\"\"\n",
    "    \n",
    "    stayDates = tbl.Date.drop_duplicates().sort_values() # Drop duplicates since stay is one or more sleep.\n",
    "    gapVals = stayDates.diff().astype('timedelta64[D]')\n",
    "    gapInd = (gapVals >= episodeGap).astype('int').cumsum().drop_duplicates(keep='first')\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Date': tbl.loc[gapInd.index].Date, # Date of first day of each episode.\n",
    "        'Ind': range(1,len(gapInd)+1)       # Episode index.\n",
    "    })\n",
    "\n",
    "\n",
    "def TimeWinThresholdTest(tbl,posFlag,negFlag,thresh,winSzDays):\n",
    "    \"\"\"Analyze a subject timeline and determine if the number of events\n",
    "    exceed thresh in a time window of winSzDays.\n",
    "    - idDate is the date the threshold test is satisfied.\n",
    "    - reqTime is the number of days it took to satisfy the threshold test.\n",
    "    - If the test is satisfied, return a series with posFlag, idDate and reqTime.\n",
    "    - If the test is not satisfied, return a series with negFlag and nan values\n",
    "      for idDate and reqTime.\"\"\"\n",
    "    \n",
    "    win = tbl.rolling('{:d}d'.format(winSzDays),on='Date').count().Ind\n",
    "    \n",
    "    registrationDate = tbl.Date.min()\n",
    "    idDate = tbl[win >= thresh].Date.min()  # Will be equal to NaN if the threshold isn't met.\n",
    "    reqTime = (idDate - registrationDate).days\n",
    "    \n",
    "    if idDate == idDate:   # Satisfied if idDate is not NaN.\n",
    "        return pd.Series({\n",
    "            'Flag': posFlag,  \n",
    "            'Date': idDate,  # Date subject was identified.\n",
    "            'Time': reqTime  # Number of days it took to identify subject.\n",
    "        })\n",
    "    else:\n",
    "        return pd.Series({   # Returned if the test is not satisfied.\n",
    "            'Flag': negFlag,\n",
    "            'Date': pd.NaT,\n",
    "            'Time': np.nan\n",
    "        })\n",
    "\n",
    "    \n",
    "def ChooseEarliestTest(test1,test2):\n",
    "    \"\"\"Merges two test tables.  If each test is positive for a subject, the test that\n",
    "    occurs earliest in a subject's timeline is chosen.  \n",
    "    \n",
    "    If you have more than two test tables, you can call this routine several times.  For \n",
    "    example, tables A, B and C can be merged by:\n",
    "      mrg = ChooseEarliestTest(A,B)\n",
    "      mrg = ChooseEarliestTest(mrg,C)\n",
    "      \n",
    "    Assumptions:\n",
    "    - Both test tables contain the identical list of subjects.\n",
    "    - Both tests use the same flag for a negative result.\n",
    "    \"\"\"\n",
    "    nRec = len(test1.index)\n",
    "    \n",
    "    tbl = pd.DataFrame({ 'Flag': ['']*nRec, 'Date': [pd.NaT]*nRec, 'Time': [np.nan]*nRec },index=test1.index)\n",
    "\n",
    "    bothNeg = (test1.Time != test1.Time) & (test2.Time != test2.Time)\n",
    "    tbl[bothNeg] = test1[bothNeg]\n",
    "    \n",
    "    isOne = (test1.Time == test1.Time) & (test2.Time == test2.Time) & (test1.Time < test2.Time)\n",
    "    isOne = isOne | ( (test1.Time == test1.Time) & (test2.Time != test2.Time) )\n",
    "    tbl[isOne] = test1[isOne]\n",
    "    \n",
    "    isTwo = (test1.Time == test1.Time) & (test2.Time == test2.Time) & (test1.Time >= test2.Time)\n",
    "    isTwo = isTwo | ( (test2.Time == test2.Time) & (test1.Time != test1.Time) )\n",
    "    tbl[isTwo] = test2[isTwo]\n",
    "    \n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf = pd.read_hdf('validClientsDf.h5') # Loading valid clients dataframe after censoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf.head(1).transpose().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First shelter access dates\n",
    "- To find the Registration Date or the first shelter access date for each client\n",
    "- Note: Registration date is Not the actual registration date, it is used in code for convinence of naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindRegistrationDates(tbl):\n",
    "    dd1 = min(tbl.Date)\n",
    "    return pd.Series({\n",
    "        'FirstAcessDate': dd1\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf2 = validClientsDf[validClientsDf[\"EntryType\"] == \"Sleep\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regDates = validClientsDf2.groupby(\"ClientId\").progress_apply(FindRegistrationDates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regDates.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date after 90 days from First Shelter Access \n",
    "1. Find the date after 90 days of First shelter access for each client \n",
    "2. Adding the date after 90 days from first interaction to the data frame\n",
    "3. Adding the first date of shelter access of each client to the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindDateAfterFirst90Days(tbl):\n",
    "    dd = min(tbl.Date)+np.timedelta64(90, 'D')\n",
    "    return pd.Series({\n",
    "        'DateAfterFirst90Days': dd\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateAfterFirst90Days = validClientsDf2.groupby(\"ClientId\").progress_apply(FindDateAfterFirst90Days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateAfterFirst90Days.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(validClientsDf, dateAfterFirst90Days, on='ClientId')\n",
    "merged_df = pd.merge(merged_df, regDates, on='ClientId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf2 = validClientsDf2.join(dateAfterFirst90Days, on='ClientId', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf2 = validClientsDf2.join(regDates, on='ClientId', how ='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test case #############\n",
    "#validClientsDf[validClientsDf.ClientId == 2493929] \n",
    "## To make sure the date after first 90 days is same across all entries of a particular client -- (Yes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "- To filter Client's data for the first 90 days of each client's shelter interaction data entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf = merged_df[merged_df.Date <= merged_df.DateAfterFirst90Days]\n",
    "validClientsFirst90DaysDf = validClientsFirst90DaysDf[validClientsFirst90DaysDf.FirstAcessDate<= validClientsFirst90DaysDf.Date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf[validClientsFirst90DaysDf.BarDuration.notnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Test case #############\n",
    "#validClientsFirst90DaysDf[validClientsFirst90DaysDf.ClientId == 2493929]\n",
    "## To make sure filtering is correct -- the entry after the first 90 days is removed (Yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf.shape  # Shape of this dataframe would be smaller than the one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsDf.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Saving the filtered records of first 90 days to disk</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf.loc[:,'Location'] = validClientsFirst90DaysDf['Location'].astype(str)\n",
    "validClientsFirst90DaysDf.loc[:,'EntryType'] = validClientsFirst90DaysDf['EntryType'].astype(str)\n",
    "validClientsFirst90DaysDf.loc[:,'ClientState'] = validClientsFirst90DaysDf['ClientState'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf = validClientsFirst90DaysDf.sort_values(['ClientId', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf[validClientsFirst90DaysDf.BarDuration.notnull()].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validClientsFirst90DaysDf.to_hdf('validClientsFirst90DaysDf2.h5',key='df',mode='w')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9f3ba9e726005c067fbc0eaab20e31f21cd7f7e132fec231b7dd4c5a2981fed5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
